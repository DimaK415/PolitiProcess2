{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import pytz\n",
    "\n",
    "import praw\n",
    "from textblob import TextBlob\n",
    "from newspaper import Article\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from Paramerator import parameters\n",
    "\n",
    "def utc_to_pacific(utc_dt):\n",
    "    local_tz = pytz.timezone('America/Los_Angeles')\n",
    "    # os.environ['TZ'] = 'America/Los_Angeles'\n",
    "    local_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz)\n",
    "    return local_tz.normalize(local_dt)\n",
    "\n",
    "def scraper(df = True):\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    params = parameters()\n",
    "\n",
    "    scraper_p  = params.loader('dat/scraper.cfg')\n",
    "    reddit     = scraper_p.RedditParams\n",
    "    art_ignore = scraper_p.Article.NoneArticleLinks\n",
    "    API        = params.loader('dat/praw.secret').APIScriptKeys\n",
    "        \n",
    "    api = praw.Reddit(client_id      = API.client_id,\n",
    "                      client_secret  = API.client_secret,\n",
    "                      password       = API.password,\n",
    "                      user_agent     = API.user_agent,\n",
    "                      username       = API.username)\n",
    "    \n",
    "    posts_dict = {\"post title\"        : [],\n",
    "                  \"subreddit\"         : [],\n",
    "                  \"score\"             : [],\n",
    "                  \"is article\"        : [],\n",
    "                  \"article title\"     : [],\n",
    "                  \"title polarity\"    : [],\n",
    "                  \"title objectivity\" : [],\n",
    "                  \"keywords\"          : [],\n",
    "                  \"domain\"            : [],\n",
    "                  \"link\"              : [],\n",
    "                  \"author\"            : [],\n",
    "                  \"text\"              : [],\n",
    "                  \"comments\"          : [],\n",
    "                  \"date\"              : [],\n",
    "                  \"target\"            : [],\n",
    "                   }\n",
    "    \n",
    "    article_count   = 0\n",
    "    invalid_links   = 0\n",
    "    failed_links_c  = 0\n",
    "    failed_links    = []\n",
    "    red_sub         = 0\n",
    "    blue_sub        = 0\n",
    "    \n",
    "    \n",
    "    for sub in reddit.RedList + reddit.BlueList:\n",
    "        submissions = (x for x in api.subreddit(sub).hot(limit=reddit.ScraperDepthLimit) if not x.stickied)\n",
    "        \n",
    "        for post in submissions:\n",
    "            \n",
    "            if sub in reddit.RedList:\n",
    "                posts_dict[\"target\"].append(True)\n",
    "                red_sub += 1\n",
    "            if sub in reddit.BlueList:\n",
    "                blue_sub += 1\n",
    "                posts_dict[\"target\"].append(False)\n",
    "           \n",
    "            posts_dict[\"post title\"].append(post.title)           # praw reddit scraping to dict\n",
    "            posts_dict[\"link\"].append(post.url)\n",
    "            posts_dict[\"score\"].append(int(post.score))\n",
    "            posts_dict[\"subreddit\"].append(sub)\n",
    "            posts_dict[\"date\"].append(datetime.fromtimestamp(post.created_utc))\n",
    "            \n",
    "            comments = []                                         # Comments parsing and scoring \n",
    "            for comment in post.comments:\n",
    "                if comment.author != 'AutoModerator':\n",
    "                    comments.append((round(comment.score/(post.num_comments), 2), comment.body))\n",
    "            posts_dict[\"comments\"].append(comments)\n",
    "            \n",
    "            parsed_url = urlparse(post.url)                       # Parse URL for domain \n",
    "            posts_dict['domain'].append(parsed_url.netloc)\n",
    "            \n",
    "            post_blob = TextBlob(post.title)                      # TextBlob NLP - VERY SIMPLE \n",
    "            posts_dict[\"title polarity\"].append(post_blob.sentiment[0])\n",
    "            posts_dict[\"title objectivity\"].append(post_blob.sentiment[1])\n",
    "            posts_dict[\"keywords\"].append(post_blob.noun_phrases)\n",
    "            \n",
    "            \n",
    "            article = Article(post.url)                                     # Instantiate newspaper3k library\n",
    "            if article.is_valid_url() and parsed_url.netloc not in art_ignore:\n",
    "                \n",
    "                try:                                                        # Try to download and parse article\n",
    "                    article.download()\n",
    "                    article.parse()\n",
    "                    \n",
    "                    article_count += 1\n",
    "                    posts_dict[\"is article\"].append(True)\n",
    "                    \n",
    "                    if article.title != []:                                  # Title parsed? \n",
    "                        posts_dict[\"article title\"].append(article.title)\n",
    "                    else:\n",
    "                        posts_dict[\"article title\"].append(np.nan)\n",
    "                    \n",
    "                    if article.authors != []:                                # Author parsed?\n",
    "                        posts_dict[\"author\"].append(article.authors)\n",
    "                    else:\n",
    "                        posts_dict[\"author\"].append(np.nan)\n",
    "                        \n",
    "                    if article.text != []:                                   # Text parsed?\n",
    "                        posts_dict['text'].append(article.text)\n",
    "                    else:\n",
    "                        posts_dict[\"text\"].append(np.nan)\n",
    "\n",
    "                except:                               \n",
    "                    posts_dict[\"is article\"].append(False)\n",
    "                    posts_dict[\"article title\"].append(np.nan)\n",
    "                    posts_dict[\"author\"].append(np.nan)\n",
    "                    posts_dict[\"text\"].append(np.nan)\n",
    "                    failed_links_c +=1\n",
    "                    failed_links.append(post.url)\n",
    "                        \n",
    "            else:\n",
    "                invalid_links += 1\n",
    "                posts_dict[\"is article\"].append(False)\n",
    "                posts_dict[\"article title\"].append(np.nan)\n",
    "                posts_dict[\"author\"].append(np.nan)\n",
    "                posts_dict[\"text\"].append(np.nan)\n",
    "                    \n",
    "    time_now = utc_to_pacific(datetime.now())                           # Set local Time\n",
    "    log_date = time_now.strftime('%m%d%y_%H%M')\n",
    "    \n",
    "    \n",
    "    if df:\n",
    "        \n",
    "        posts_df = pd.DataFrame(posts_dict)                             # Make it a dataframe\n",
    "        posts_df =posts_df[[\"subreddit\",\n",
    "                            \"post title\",\n",
    "                            \"title polarity\",\n",
    "                            \"title objectivity\",\n",
    "                            \"score\",\n",
    "                            \"keywords\",\n",
    "                            \"comments\",\n",
    "                            \"domain\", \n",
    "                            \"link\",\n",
    "                            \"is article\",\n",
    "                            \"article title\",\n",
    "                            \"author\",\n",
    "                            \"text\",\n",
    "                            \"date\", \n",
    "                            \"target\"\n",
    "                           ]]\n",
    "        \n",
    "        \n",
    "        posts_df.to_pickle(f'log/{log_date}.pickle')\n",
    "        \n",
    "    z = datetime.now() - start_time\n",
    "    scrape_time = f\"{(z.seconds//60)%60}min, {z.seconds%60}sec\"\n",
    "    \n",
    "    log = params.loader('log/empty_scraper.log')\n",
    "    \n",
    "    log.ScraperLog.Date               = time_now.ctime()\n",
    "    log.ScraperLog.Scraper_Timer      = scrape_time\n",
    "    log.ScraperLog.Article_Count      = article_count \n",
    "    log.ScraperLog.Invalid_Links      = invalid_links \n",
    "    log.ScraperLog.Failed_Links       = failed_links \n",
    "    log.ScraperLog.Failed_Links_Count = failed_links_c\n",
    "    log.ScraperLog.Red_Sub_Count      = red_sub       \n",
    "    log.ScraperLog.Blue_Sub_Count     = blue_sub      \n",
    "    \n",
    "    params.writer(f'log/{log_date}_scraper.log', log)\n",
    "    \n",
    "    if df: \n",
    "        return posts_df\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
