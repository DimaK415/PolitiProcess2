{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Library Imports\n",
    "print(\"Loading Libraries\")\n",
    "\n",
    "# Standard Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# MongoDB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# NLP Libraries\n",
    "import textacy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Timezone Info - TO DO - Fix Docker container for matching timezone\n",
    "local_tz = pytz.timezone('America/Los_Angeles')\n",
    "os.environ['TZ'] = 'America/Los_Angeles'\n",
    "\n",
    "print(\"Loading defs\")\n",
    "\n",
    "def utc_to_local(utc_dt):\n",
    "    local_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz)\n",
    "    return local_tz.normalize(local_dt)\n",
    "\n",
    "utc_to_local(datetime.now())\n",
    "\n",
    "## Import Parameters from Topic_Extractor_Params.dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Assign Params from .dats ##\n",
    "topic_params =    param_loader('Topic_Extractor_Params.dat')\n",
    "mongo_params =    param_loader('../Scraper_Params.dat')\n",
    "\n",
    "## Stop Words ##\n",
    "stop_words_list = stop_loader('Stop_Words_List.dat')\n",
    "\n",
    "# ## Mongo Params ##\n",
    "# mongo_host        = str(mongo_params['mongodb_host'])\n",
    "# mongo_port        = int(mongo_params['mongodb_port'])\n",
    "# mongo_db          = str(mongo_params['db'])\n",
    "# mongo_collection  = str(mongo_params['collection'])\n",
    "\n",
    "# ## Query Params ##\n",
    "# n_hours           = int(topic_params['Time Frame in Hours'])\n",
    "# red_or_blue       = str(topic_params['Red, Blue or All'])\n",
    "\n",
    "# ## Cleaner Params ##\n",
    "# fix_unicode       = bool(int(topic_params['Fix Unicode']))\n",
    "# lowercase         = bool(int(topic_params['All Lowercase']))\n",
    "# no_accents        = bool(int(topic_params['Remove Accents']))\n",
    "# no_contradictions = bool(int(topic_params['Remove Contradictions']))\n",
    "# no_emails         = bool(int(topic_params['Remove Emails']))\n",
    "# no_newline        = bool(int(topic_params['Remove Newline']))\n",
    "# no_punctuation    = bool(int(topic_params['Remove Punctuation']))\n",
    "# no_currency       = bool(int(topic_params['Replace Currency']))\n",
    "\n",
    "# ## Spacy Params ##\n",
    "# spacy_model       = str(topic_params['Spacy Model'])\n",
    "# min_word_length   = int(topic_params['Min Word Length'])\n",
    "# use_cleaned       = bool(int(topic_params['Use Cleaned Text']))\n",
    "# split_columns     = bool(int(topic_params['Split Columns']))\n",
    "# named_entities    = topic_params['Named Entity List'].split(' ')\n",
    "\n",
    "# ## TFIDF Params ## \n",
    "# if bool(int(topic_params['Use IDF'])):\n",
    "#     weighting     = 'tfidf'\n",
    "# else:\n",
    "#     weighting     = 'tf'\n",
    "    \n",
    "# if topic_params['Max Terms'] == '0':\n",
    "#     max_n_terms   = None\n",
    "# else:\n",
    "#     max_n_terms   = int(topic_params['Max Terms'])\n",
    "    \n",
    "# column            = str(topic_params['Column to Vectorize'])\n",
    "# normalize         = bool(int(topic_params['Normalize']))\n",
    "# sublinear_tf      = bool(int(topic_params['Sublinear TF']))\n",
    "# smooth_idf        = bool(int(topic_params['Smooth IDF']))\n",
    "# vocabulary        = topic_params['Vocabulary']\n",
    "# min_df            = float(topic_params['Min DF'])\n",
    "# max_df            = float(topic_params['Max DF'])\n",
    "# min_ic            = float(topic_params['Min IC'])\n",
    "\n",
    "# ## Decomposition Params ##\n",
    "# n_topics          = int(topic_params['Number of Topics'])\n",
    "# model_type        = str(topic_params['Model Type'])\n",
    "\n",
    "# ## Visualization Params ##\n",
    "# top_n_terms       = topic_params['Top Terms Per Topic']\n",
    "# sort_terms_by     = topic_params['Sort Terms By']\n",
    "# term_depth        = topic_params['Depth of Termite Plot']\n",
    "# highlight         = topic_params['Highlight']\n",
    "# directory         = topic_params['Save Directory']\n",
    "\n",
    "# try:\n",
    "#     save          = bool(int(topic_params['Save']))\n",
    "# except:\n",
    "#     save          = str(topic_params['Save'])\n",
    "\n",
    "def stop_fixer(file, upper=True, no_punct=True, fix_in_place=False):\n",
    "    \n",
    "    '''Reads and parses *file* - Returns list of stops or (if fix_in_place) overwrites existing *file*\n",
    "    (if upper) adds capitlized version of stop words\n",
    "    (if no_punct) adds unpunctuated version of stop words'''\n",
    "    \n",
    "    vocab_dict = stop_loader('Stop_Words_List.dat', as_list = False)\n",
    "    \n",
    "    for key in vocab_dict:\n",
    "        if no_punct or upper:\n",
    "            for word in vocab_dict[key]:\n",
    "                if no_punct:\n",
    "                    word_punct = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "                    if word_punct not in vocab_dict[key]:\n",
    "                        vocab_dict[key].append(word_punct)\n",
    "                \n",
    "                if upper:\n",
    "                    word_caps = word.capitalize()\n",
    "                    if word_caps not in vocab_dict[key]:\n",
    "                        vocab_dict[key].append(word_caps)\n",
    "    \n",
    "       \n",
    "        sorted(vocab_dict[key], key=str.lower)\n",
    "        vocab_dict[key] = sorted(vocab_dict[key], key=str.lower)\n",
    "        \n",
    "    if fix_in_place:\n",
    "        \n",
    "        writer = open(file, mode='w+')\n",
    "        \n",
    "        for key in vocab_dict:\n",
    "            writer.writelines('\\n' + key + '\\n')\n",
    "            for word in vocab_dict[key]:\n",
    "                writer.writelines(word  + '\\n')\n",
    "    else:        \n",
    "        return vocab_dict\n",
    "\n",
    "def spacy_stopword_adder():\n",
    "    print(f\"Adding {len(stop_words_list)} custom stop words to Spacy Model {spacy_model}.\")\n",
    "    \n",
    "    if not 'nlp' in globals():\n",
    "        print(f\"Loading Spacy Model {spacy_model}.  This could take a while...\")\n",
    "        global nlp\n",
    "        nlp = spacy.load(spacy_model)\n",
    "        print(\"Complete\")\n",
    "    \n",
    "    for stopword in stop_words_list:\n",
    "        STOP_WORDS.add(stopword)\n",
    "        \n",
    "    nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "    print(f\"Complete. There are {len(STOP_WORDS)} stop words in the list.\")\n",
    "    \n",
    "\n",
    "def mongo_query(host=mongo_host, port=mongo_port, mongo_db=mongo_db, mongo_collection=mongo_collection, \n",
    "                articles=True, custom_query=None, n_hours = n_hours, red_or_blue = red_or_blue, ALL=False):\n",
    "\n",
    "    if custom_query and ALL:\n",
    "        raise ValueError(\"Cannot have custom query and ALL query at once.\")\n",
    "    \n",
    "    client = MongoClient(host=mongo_host, port=mongo_port)\n",
    "    \n",
    "    db = getattr(client, mongo_db)\n",
    "    \n",
    "    collection = getattr(db, mongo_collection)\n",
    "    \n",
    "    if ALL:\n",
    "        df = pd.DataFrame(list(collection.find()))\n",
    "        return df\n",
    "    \n",
    "    if custom_query:\n",
    "        df = pd.DataFrame(list(collection.find(custom_query)))\n",
    "        return df\n",
    "    \n",
    "    if articles:\n",
    "        post = 'articles'\n",
    "    else:\n",
    "        post = 'documents'\n",
    "    \n",
    "    query = {'is article': articles}\n",
    "    \n",
    "    if n_hours == 0:\n",
    "        print(f\"Pulling all articles from {red_or_blue} targets.\")\n",
    "    else:\n",
    "        print(f\"Pulling {red_or_blue} articles from last {n_hours} hours.\")\n",
    "        dt = datetime.utcnow() - timedelta(hours=n_hours)\n",
    "        query['date'] = {'$gt': dt}\n",
    "    \n",
    "    if red_or_blue == 'Red':\n",
    "        query['target'] = True\n",
    "    elif red_or_blue == 'Blue':\n",
    "        query['target'] = False\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(list(collection.find(query)))\n",
    "    \n",
    "    print(f'''Completed pulling {len(df)} {post}.\n",
    "    Latest article is from {collection.find_one(sort=[('date', -1)])['date']} UTC''')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pre_processor(column = 'text'):\n",
    "    \n",
    "    cleaned = [] \n",
    "    for x in range(len(df)):\n",
    "        text = textacy.preprocess.preprocess_text(df[column][x],\n",
    "                                            fix_unicode=fix_unicode,\n",
    "                                            lowercase=lowercase,\n",
    "                                            no_punct=no_punctuation,\n",
    "                                            no_contractions=no_contradictions,\n",
    "                                            no_currency_symbols=no_currency,\n",
    "                                            no_emails=no_emails,\n",
    "                                            no_accents=no_accents)\n",
    "        if no_newline:\n",
    "            text = text.replace('\\n', ' ') \n",
    "        \n",
    "        cleaned.append(text)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def spacy_processing(spacy_model=spacy_model, use_cleaned = use_cleaned, split_columns = split_columns):\n",
    "    \n",
    "    if 'nlp' not in globals():\n",
    "        print(f\"Loading Spacy Model {spacy_model}.  This could take a while...\")\n",
    "        global nlp\n",
    "        nlp = spacy.load(spacy_model)\n",
    "        print(\"Complete\")\n",
    "    \n",
    "    if use_cleaned:\n",
    "        corpus    = 'cleaned'\n",
    "        df_column = 'cleaned'\n",
    "    else:\n",
    "        corpus    = 'raw'\n",
    "        df_column = 'text'   \n",
    "    \n",
    "    print(f'''Filtering stops and words shorter than {min_word_length + 1} letters. \n",
    "Chunking and identifying {named_entities} entities from {corpus} corpus.''')    \n",
    "    \n",
    "    chunks_list = []\n",
    "    ents_list   = []\n",
    "    \n",
    "    for text in df[df_column]:\n",
    "        \n",
    "        doc = nlp(str(text))\n",
    "        \n",
    "        chunks = []\n",
    "        ents   = []\n",
    "        \n",
    "        for span in doc.noun_chunks:\n",
    "            if len(span) == 1:\n",
    "                if span[0].is_stop or len(span[0]) <= min_word_length:\n",
    "                    continue\n",
    "                else:\n",
    "                    chunks.append(span.text)\n",
    "                    continue\n",
    "            else:\n",
    "                chunks.append(span.text)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in named_entities:\n",
    "                ents.append(ent.text)\n",
    "                \n",
    "        chunks_list.append(chunks)\n",
    "        ents_list.append(ents)\n",
    "    \n",
    "    if split_columns:\n",
    "        df['chunks'], df['ents'] = chunks_list, ents_list\n",
    "        del chunks_list, ents_list\n",
    "        print(f'''Done inserting {len(df.chunks.sum())} chunks and\n",
    "              {len(df.ents.sum())} entities into df.chunks and df.ents.''')\n",
    "    else:\n",
    "        joined_list = [a + b for a, b in zip(chunks_list, ents_list)]\n",
    "        del chunks_list, ents_list\n",
    "        df['chunks_ents'] = joined_list\n",
    "        del joined_list\n",
    "        print(f\"Done inserting {len(df.chunks_ents.sum())} chunks and entities into df.chunks_ents.)\")\n",
    "\n",
    "def topic_modeler(save=True, visualize=True, **kwargs):    \n",
    "    \n",
    "    print(f\"Loading Vectorizer Paramaters\")\n",
    "    \n",
    "    vocabulary=None\n",
    "    \n",
    "    vectorizer = textacy.vsm.Vectorizer(weighting, normalize, sublinear_tf,\n",
    "                                        smooth_idf, vocabulary, min_df, max_df,\n",
    "                                        min_ic, max_n_terms)\n",
    "    \n",
    "    print(f\"Fitting Vectorizor on {column} column\")\n",
    "    \n",
    "    doc_term_matrix = vectorizer.fit_transform(df[column])\n",
    "    \n",
    "    model = textacy.tm.TopicModel(model_type, n_topics=n_topics)\n",
    "    model.fit(doc_term_matrix)\n",
    "    \n",
    "    topics_string = \"\"\n",
    "    \n",
    "    for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=top_n_terms):\n",
    "        topics_string += ' - '.join(top_terms) + ' '\n",
    "        print('topic', topic_idx, ':', '   '.join(top_terms))\n",
    "        \n",
    "    if visualize:\n",
    "        if isinstance(save, str):\n",
    "            save = save\n",
    "        elif save is True:\n",
    "            time_stamp = str(utc_to_local(datetime.now()).strftime('%m_%d_%y_%H_%M'))\n",
    "            save = f\"{directory}/{n_hours}hr_{red_or_blue}_{column}_{time_stamp}\"\n",
    "        \n",
    "        model.termite_plot(doc_term_matrix, vectorizer.id_to_term, highlight_topics=highlight,\n",
    "                       topics=-1,  n_terms=term_depth, sort_terms_by=sort_terms_by, save=save)\n",
    "\n",
    "## Import Database from Mongo\n",
    "df = mongo_query()\n",
    "\n",
    "## Preprocessing\n",
    "df['cleaned'] = pre_processor()\n",
    "\n",
    "## Adding Stop Words to spacy\n",
    "spacy_stopword_adder()\n",
    "\n",
    "## Spacy Tokenization and Entity Recognition\n",
    "spacy_processing()\n",
    "\n",
    "## Run Topic Modeler and Visualize Results\n",
    "topic_modeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/9f/22b9dec63bff5e6ef7fb47b2cd37025087c3995b6ca5467d78160f5b0eb3/textacy-0.6.1-py2.py3-none-any.whl (137kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 2.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx>=1.11 in /opt/conda/lib/python3.6/site-packages (from textacy) (2.1)\n",
      "Collecting pyphen>=0.9.4 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/c4/74859f895e2361d92cfbb6208ea7afd06c2f1f05c491da71cbd7ce3887be/Pyphen-0.9.4-py2.py3-none-any.whl (1.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cytoolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from textacy) (0.9.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from textacy) (0.18.2)\n",
      "Collecting cachetools>=2.0.0 (from textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/e8/5492fd5ada0b05a1bc485bcb634b559acdec59383eef5c4203b5e22be296/cachetools-2.0.1-py2.py3-none-any.whl\n",
      "Collecting python-levenshtein>=0.12.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /opt/conda/lib/python3.6/site-packages (from textacy) (2.18.4)\n",
      "Collecting unidecode>=0.04.19 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 3.1MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting ijson>=2.3 (from textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/e9/8508c5f4987ba238a2b169e582c1f70a47272b22a2f1fb06b9318201bb9e/ijson-2.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from textacy) (0.19.1)\n",
      "Collecting spacy>=2.0.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/31/e60f88751e48851b002f78a35221d12300783d5a43d4ef12fbf10cca96c3/spacy-2.0.11.tar.gz (17.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.6MB 144kB/s eta 0:00:01   13% |████▎                           | 2.4MB 7.1MB/s eta 0:00:03    27% |████████▊                       | 4.8MB 4.8MB/s eta 0:00:03    64% |████████████████████▋           | 11.3MB 8.1MB/s eta 0:00:01    65% |████████████████████▉           | 11.4MB 880kB/s eta 0:00:07    73% |███████████████████████▊        | 13.0MB 4.3MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting pyemd>=0.3.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/c5/7fea8e7a71cd026b30ed3c40e4c5ea13a173e28f8855da17e25271e8f545/pyemd-0.5.1.tar.gz (91kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ftfy<5.0.0,>=4.2.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.11.1 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/bc/de067ab2d700b91717dc5459d86a1877e2df31abfb90ab01a5a5a5ce30b4/tqdm-4.23.0-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from textacy) (1.12.1)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=1.11->textacy) (4.2.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from cytoolz>=0.8.0->textacy) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from python-levenshtein>=0.12.0->textacy) (39.0.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.10.0->textacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.10.0->textacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.10.0->textacy) (2018.1.18)\n",
      "Collecting murmurhash<0.29,>=0.28 (from spacy>=2.0.0->textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/31/c8c1ecafa44db30579c8c457ac7a0f819e8b1dbc3e58308394fff5ff9ba7/murmurhash-0.28.0.tar.gz\n",
      "Collecting cymem<1.32,>=1.30 (from spacy>=2.0.0->textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/9e/273fbea507de99166c11cd0cb3fde1ac01b5bc724d9a407a2f927ede91a1/cymem-1.31.2.tar.gz\n",
      "Collecting preshed<2.0.0,>=1.0.0 (from spacy>=2.0.0->textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/ac/7c17b1fd54b60972785b646d37da2826311cca70842c011c4ff84fbe95e0/preshed-1.0.0.tar.gz (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 2.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting thinc<6.11.0,>=6.10.1 (from spacy>=2.0.0->textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/fd/e9f36081e6f53699943381858848f3b4d759e0dd03c43b98807dde34c252/thinc-6.10.2.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 1.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy>=2.0.0->textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting pathlib (from spacy>=2.0.0->textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/9b065a76b9af472437a0059f77e8f962fe350438b927cb80184c32f075eb/pathlib-1.0.1.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 992kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ujson>=1.35 (from spacy>=2.0.0->textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3,>=0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.0->textacy) (0.2.7.1)\n",
      "Collecting regex==2017.4.5 (from spacy>=2.0.0->textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K    100% |████████████████████████████████| 604kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: html5lib in /opt/conda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.2.0->textacy) (1.0.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.2.0->textacy) (0.1.7)\n",
      "Collecting wrapt (from thinc<6.11.0,>=6.10.1->spacy>=2.0.0->textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.1->spacy>=2.0.0->textacy) (1.11.0)\n",
      "Collecting termcolor (from thinc<6.11.0,>=6.10.1->spacy>=2.0.0->textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: msgpack-python in /opt/conda/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.1->spacy>=2.0.0->textacy) (0.5.6)\n",
      "Collecting msgpack-numpy==0.4.1 (from thinc<6.11.0,>=6.10.1->spacy>=2.0.0->textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/2e/43/393e30e2768b0357541ac95891f96b80ccc4d517e0dd2fa3042fc8926538/msgpack_numpy-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy) (0.5)\n",
      "Building wheels for collected packages: python-levenshtein, spacy, pyemd, ftfy, murmurhash, cymem, preshed, thinc, pathlib, ujson, regex, wrapt, termcolor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running setup.py bdist_wheel for python-levenshtein ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
      "  Running setup.py bdist_wheel for spacy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/fb/00/28/75c85d5135e7d9a100639137d1847d41e914ed16c962d467e4\n",
      "  Running setup.py bdist_wheel for pyemd ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/e4/ba/b0/1f4178a35c916b22fc51dc56f278125d4b8cfb0592e5f0cc24\n",
      "  Running setup.py bdist_wheel for ftfy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
      "  Running setup.py bdist_wheel for murmurhash ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/b8/94/a4/f69f8664cdc1098603df44771b7fec5fd1b3d8364cdd83f512\n",
      "  Running setup.py bdist_wheel for cymem ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/55/8d/4a/f6328252aa2aaec0b1cb906fd96a1566d77f0f67701071ad13\n",
      "  Running setup.py bdist_wheel for preshed ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/8f/85/06/2d132fb649a6bbcab22487e4147880a55b0dd0f4b18fdfd6b5\n",
      "  Running setup.py bdist_wheel for thinc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/d8/5c/3e/9acf5d9974fb1c9e7b467563ea5429c9325f67306e93147961\n",
      "  Running setup.py bdist_wheel for pathlib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/f9/b2/4a/68efdfe5093638a9918bd1bb734af625526e849487200aa171\n",
      "  Running setup.py bdist_wheel for ujson ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
      "  Running setup.py bdist_wheel for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built python-levenshtein spacy pyemd ftfy murmurhash cymem preshed thinc pathlib ujson regex wrapt termcolor\n",
      "\u001b[31mipywidgets 7.1.2 has requirement widgetsnbextension~=3.1.0, but you'll have widgetsnbextension 3.2.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mthinc 6.10.2 has requirement cytoolz<0.9,>=0.8, but you'll have cytoolz 0.9.0.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pyphen, cachetools, python-levenshtein, unidecode, ijson, murmurhash, cymem, preshed, wrapt, tqdm, plac, termcolor, pathlib, msgpack-numpy, thinc, ujson, regex, spacy, pyemd, ftfy, textacy\n",
      "Successfully installed cachetools-2.0.1 cymem-1.31.2 ftfy-4.4.3 ijson-2.3 msgpack-numpy-0.4.1 murmurhash-0.28.0 pathlib-1.0.1 plac-0.9.6 preshed-1.0.0 pyemd-0.5.1 pyphen-0.9.4 python-levenshtein-0.12.0 regex-2017.4.5 spacy-2.0.11 termcolor-1.1.0 textacy-0.6.1 thinc-6.10.2 tqdm-4.23.0 ujson-1.35 unidecode-1.0.22 wrapt-1.10.11\n"
     ]
    }
   ],
   "source": [
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5211c23c6909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dat'"
     ]
    }
   ],
   "source": [
    "from dat import defaults\n",
    "\n",
    "from ast import literal_eval\n",
    "from collections import namedtuple, OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "class parameters:\n",
    "\n",
    "    def __init__(self, stop=True, from_file=True, verbose=False):\n",
    "        \n",
    "        import dat\n",
    "        \n",
    "        self.defaults   = dat.defaults\n",
    "        self.params     = self.param_loader(self.defaults['PARAMS']['Default Path'])\n",
    "        self.mongo      = self.param_loader(self.defaults['MONGO']['Default Path'])\n",
    "        \n",
    "            \n",
    "    def param_writer(self, file, params, default_location= True, overwrite=False):\n",
    "        '''A writer definition for writing new param files and overwriting old ones.  USE WITH CAUTION!'''\n",
    "        \n",
    "        from pathlib import Path\n",
    "        \n",
    "        if Path(file).is_file():\n",
    "            overwrite = literal_eval(input(f'{file} already exists. Overwrite?  :'))\n",
    "            if overwrite:\n",
    "                pass\n",
    "            else:\n",
    "                file = input('/path/to/new.file :')\n",
    "\n",
    "        writer = open(file, mode='w+')\n",
    "        \n",
    "        for key in params:\n",
    "            writer.writelines('\\n' + '{}'.format(key) + '\\n')\n",
    "            for value in params[key]:\n",
    "                \n",
    "                writer.writelines('{:30}{}{}'.format(value, '= ', params[key][value]) + '\\n')\n",
    "        print(f\"Completed writing {list(params.keys())} to '{file}'.\")\n",
    "    \n",
    "    def param_loader(self, file):\n",
    "        '''A loader definition for loading script dependant parameters from file'''\n",
    "        \n",
    "        self.file = file\n",
    "        \n",
    "        from pathlib import Path\n",
    "        \n",
    "        if not Path(file).is_file():\n",
    "            response = literal_eval(input(f'\"{file}\" not found.  Would you like to attempt to generate a default?'))\n",
    "            if response:\n",
    "                for key in self.dd:\n",
    "                    if self.dd[key]['Default Path'] == file:\n",
    "                        response = literal_eval(input(f'Found Default \"{file}\". Generate?'))\n",
    "                        if response:\n",
    "                            dft_dict = self.dd[key]['DAT']\n",
    "                            self.param_writer(file, dft_dict)\n",
    "            \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        fileObj = open(file, mode='r')\n",
    "            \n",
    "        params_dict  = OrderedDict()\n",
    "        \n",
    "        for line in fileObj:\n",
    "            if not line:\n",
    "                pass\n",
    "            line = line.strip()\n",
    "            key_value = line.split('=')\n",
    "    \n",
    "            if \"#\" in line:\n",
    "                section = line[3:].replace(\" \", \"\")\n",
    "                params_dict[section] = {}\n",
    "\n",
    "            if len(key_value) == 2:\n",
    "                key = key_value[0].strip().replace(\" \", \"\")\n",
    "                value = key_value[1].strip()\n",
    " \n",
    "                if not value:\n",
    "                    value = input(f'Required parameter \"{key_value[0].strip()}\" is missing: ')\n",
    "                    while not value:\n",
    "                        value = input(f'You must enter a value for {key_value[0].strip()}: ')\n",
    "                    \n",
    "                \n",
    "                if \",\" in value:\n",
    "                    try:\n",
    "                        params_dict[section][key] = list(literal_eval(value))\n",
    "                    except:\n",
    "                        params_dict[section][key] = value.split(\", \")\n",
    "                else:\n",
    "                    try:\n",
    "                        params_dict[section][key] = literal_eval(value)\n",
    "                    except:\n",
    "                        params_dict[section][key] = value\n",
    "        \n",
    "        param_nt = namedtuple('params', params_dict.keys())(**params_dict)\n",
    "        return param_nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-84d056c18876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-a637e2dd84ed>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stop, from_file, verbose)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mdat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dat'"
     ]
    }
   ],
   "source": [
    "parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Libraries\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mongo_host' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ccb6fd7856f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPolitiprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel_params\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m'Topic_Extractor_Params.dat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ccb6fd7856f8>\u001b[0m in \u001b[0;36mPolitiprocess\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     def mongo_query(host=mongo_host, port=mongo_port, mongo_db=mongo_db, mongo_collection=mongo_collection, \n\u001b[0m\u001b[1;32m    159\u001b[0m                     articles=True, custom_query=None, n_hours = n_hours, red_or_blue = red_or_blue, ALL=False):\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mongo_host' is not defined"
     ]
    }
   ],
   "source": [
    "## Library Imports\n",
    "print(\"Loading Libraries\")\n",
    "\n",
    "# Standard Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import pprint\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "from ast import literal_eval\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# MongoDB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# NLP Libraries\n",
    "import textacy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "class Politiprocess:\n",
    "\n",
    "    model_params   = 'Topic_Extractor_Params.dat'\n",
    "    mongo_params   = '../Scraper_Params.dat'\n",
    "    \n",
    "    stop_list = []\n",
    "    \n",
    "    def __init__(self, from_file=True, verbose=False):\n",
    "        print(\"Loading Params\")\n",
    "        \n",
    "        self.params = self.param_loader(self.model_params)\n",
    "        self.mongo  = self.param_loader(self.mongo_params)\n",
    "        \n",
    "        if verbose:\n",
    "            print(self.params)\n",
    "    \n",
    "    def param_loader(self, file):\n",
    "        '''A loader definition for loading script dependant parameters from file'''\n",
    "        \n",
    "        fileObj = open(file, mode='r')\n",
    "        params_dict  = OrderedDict()\n",
    "        \n",
    "        for line in fileObj:\n",
    "            if not line:\n",
    "                pass\n",
    "            line = line.strip()\n",
    "            key_value = line.split('=')\n",
    "    \n",
    "            if \"#\" in line:\n",
    "                section = line[3:].replace(\" \", \"\")\n",
    "                params_dict[section] = {}\n",
    "\n",
    "            if len(key_value) == 2:\n",
    "                key = key_value[0].strip().replace(\" \", \"\")\n",
    "                value = key_value[1].strip()\n",
    " \n",
    "                if not value:\n",
    "                    value = input(f'Required parameter \"{key_value[0].strip()}\" is missing: ')\n",
    "                    while not value:\n",
    "                        value = input(f'You must enter a value for {key_value[0].strip()}: ')\n",
    "                    \n",
    "                \n",
    "                if \",\" in value:\n",
    "                    try:\n",
    "                        params_dict[section][key] = list(literal_eval(value))\n",
    "                    except:\n",
    "                        params_dict[section][key] = value.split(\", \")\n",
    "                else:\n",
    "                    try:\n",
    "                        params_dict[section][key] = literal_eval(value)\n",
    "                    except:\n",
    "                        params_dict[section][key] = value\n",
    "        \n",
    "        param_nt = namedtuple('params', params_dict.keys())(**params_dict)\n",
    "        return param_nt\n",
    "    \n",
    "    def stop_loader(self, file='Stop_Words_List.dat', as_list=True):\n",
    "        '''A loader for loading a unicode file of stop words as dictionaries.\n",
    "        Keys are categories and Values are lists of terms'''\n",
    "        \n",
    "        fileObj = open(file, mode='r')\n",
    "        \n",
    "        vocab_dict = {}\n",
    "        vocab_list = []\n",
    "        current_key = \"\"\n",
    "        \n",
    "        for line in fileObj.read().split('\\n'):\n",
    "            \n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            if \"#\" in line:\n",
    "                vocab_list = []\n",
    "                vocab_dict[line] = []\n",
    "                current_key = str(line)\n",
    "            \n",
    "            if line[0].isalpha():\n",
    "                vocab_list.append(line)\n",
    "            \n",
    "            vocab_dict[current_key] = vocab_list\n",
    "            \n",
    "        if as_list:\n",
    "            vocab_list = []\n",
    "            for terms in vocab_dict.values():\n",
    "                vocab_list.extend(terms)\n",
    "            print(f\"Loading {len(vocab_list)} stop words from {list(vocab_dict.keys())}\")\n",
    "            return vocab_list\n",
    "        \n",
    "        return vocab_dict\n",
    "    \n",
    "    def stop_fixer(self, file='Stop_Words_List.dat', upper=True, no_punct=True, fix_in_place=False):\n",
    "        '''Reads and parses *file* - Returns list of stops or (if fix_in_place) overwrites existing *file*\n",
    "        (if upper) adds capitlized version of stop words\n",
    "        (if no_punct) adds unpunctuated version of stop words'''\n",
    "        \n",
    "        vocab_dict = self.stop_loader('Stop_Words_List.dat', as_list = False)\n",
    "        \n",
    "        for key in vocab_dict:\n",
    "            if no_punct or upper:\n",
    "                for word in vocab_dict[key]:\n",
    "                    if no_punct:\n",
    "                        word_punct = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "                        if word_punct not in vocab_dict[key]:\n",
    "                            vocab_dict[key].append(word_punct)\n",
    "                    \n",
    "                    if upper:\n",
    "                        word_caps = word.capitalize()\n",
    "                        if word_caps not in vocab_dict[key]:\n",
    "                            vocab_dict[key].append(word_caps)\n",
    "        \n",
    "           \n",
    "            sorted(vocab_dict[key], key=str.lower)\n",
    "            vocab_dict[key] = sorted(vocab_dict[key], key=str.lower)\n",
    "            \n",
    "        if fix_in_place:\n",
    "            \n",
    "            writer = open(file, mode='w+')\n",
    "            \n",
    "            for key in vocab_dict:\n",
    "                writer.writelines('\\n' + key + '\\n')\n",
    "                for word in vocab_dict[key]:\n",
    "                    writer.writelines(word  + '\\n')\n",
    "        else:        \n",
    "            return vocab_dict\n",
    "\n",
    "\n",
    "    \n",
    "#     def mongo_query(host=mongo_host, port=mongo_port, mongo_db=mongo_db, mongo_collection=mongo_collection, \n",
    "#                     articles=True, custom_query=None, n_hours = n_hours, red_or_blue = red_or_blue, ALL=False):\n",
    "    \n",
    "#         if custom_query and ALL:\n",
    "#             raise ValueError(\"Cannot have custom query and ALL.\")\n",
    "        \n",
    "#         client = MongoClient(host=mongo_host, port=mongo_port)\n",
    "        \n",
    "#         db = getattr(client, mongo_db)\n",
    "        \n",
    "#         collection = getattr(db, mongo_collection)\n",
    "        \n",
    "#         if ALL:\n",
    "#             df = pd.DataFrame(list(collection.find()))\n",
    "#             return df\n",
    "        \n",
    "#         if custom_query:\n",
    "#             df = pd.DataFrame(list(collection.find(custom_query)))\n",
    "#             return df\n",
    "        \n",
    "#         if articles:\n",
    "#             post = 'articles'\n",
    "#         else:\n",
    "#             post = 'documents'\n",
    "        \n",
    "#         query = {'is article': articles}\n",
    "        \n",
    "#         if n_hours == 0:\n",
    "#             print(f\"Pulling all articles from {red_or_blue} targets.\")\n",
    "#         else:\n",
    "#             print(f\"Pulling {red_or_blue} articles from last {n_hours} hours.\")\n",
    "#             dt = datetime.utcnow() - timedelta(hours=n_hours)\n",
    "#             query['date'] = {'$gt': dt}\n",
    "        \n",
    "#         if red_or_blue == 'Red':\n",
    "#             query['target'] = True\n",
    "#         elif red_or_blue == 'Blue':\n",
    "#             query['target'] = False\n",
    "#         else:\n",
    "#             pass\n",
    "        \n",
    "#         df = pd.DataFrame(list(collection.find(query)))\n",
    "        \n",
    "#         print(f'''Completed pulling {len(df)} {post}.\n",
    "#         Latest article is from {collection.find_one(sort=[('date', -1)])['date']} UTC''')\n",
    "        \n",
    "#         return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Politiprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(topics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=80, width = 800, height=400).generate(topics_string)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud, interpolation=\"bicubic\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
