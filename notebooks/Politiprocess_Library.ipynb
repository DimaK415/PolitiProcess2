{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Library Imports\n",
    "print(\"Loading Libraries\")\n",
    "\n",
    "# Standard Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# MongoDB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# NLP Libraries\n",
    "import textacy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Timezone Info - TO DO - Fix Docker container for matching timezone\n",
    "local_tz = pytz.timezone('America/Los_Angeles')\n",
    "os.environ['TZ'] = 'America/Los_Angeles'\n",
    "\n",
    "print(\"Loading defs\")\n",
    "\n",
    "def utc_to_local(utc_dt):\n",
    "    local_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz)\n",
    "    return local_tz.normalize(local_dt)\n",
    "\n",
    "utc_to_local(datetime.now())\n",
    "\n",
    "## Import Parameters from Topic_Extractor_Params.dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Assign Params from .dats ##\n",
    "topic_params =    param_loader('Topic_Extractor_Params.dat')\n",
    "mongo_params =    param_loader('../Scraper_Params.dat')\n",
    "\n",
    "## Stop Words ##\n",
    "stop_words_list = stop_loader('Stop_Words_List.dat')\n",
    "\n",
    "# ## Mongo Params ##\n",
    "# mongo_host        = str(mongo_params['mongodb_host'])\n",
    "# mongo_port        = int(mongo_params['mongodb_port'])\n",
    "# mongo_db          = str(mongo_params['db'])\n",
    "# mongo_collection  = str(mongo_params['collection'])\n",
    "\n",
    "# ## Query Params ##\n",
    "# n_hours           = int(topic_params['Time Frame in Hours'])\n",
    "# red_or_blue       = str(topic_params['Red, Blue or All'])\n",
    "\n",
    "# ## Cleaner Params ##\n",
    "# fix_unicode       = bool(int(topic_params['Fix Unicode']))\n",
    "# lowercase         = bool(int(topic_params['All Lowercase']))\n",
    "# no_accents        = bool(int(topic_params['Remove Accents']))\n",
    "# no_contradictions = bool(int(topic_params['Remove Contradictions']))\n",
    "# no_emails         = bool(int(topic_params['Remove Emails']))\n",
    "# no_newline        = bool(int(topic_params['Remove Newline']))\n",
    "# no_punctuation    = bool(int(topic_params['Remove Punctuation']))\n",
    "# no_currency       = bool(int(topic_params['Replace Currency']))\n",
    "\n",
    "# ## Spacy Params ##\n",
    "# spacy_model       = str(topic_params['Spacy Model'])\n",
    "# min_word_length   = int(topic_params['Min Word Length'])\n",
    "# use_cleaned       = bool(int(topic_params['Use Cleaned Text']))\n",
    "# split_columns     = bool(int(topic_params['Split Columns']))\n",
    "# named_entities    = topic_params['Named Entity List'].split(' ')\n",
    "\n",
    "# ## TFIDF Params ## \n",
    "# if bool(int(topic_params['Use IDF'])):\n",
    "#     weighting     = 'tfidf'\n",
    "# else:\n",
    "#     weighting     = 'tf'\n",
    "    \n",
    "# if topic_params['Max Terms'] == '0':\n",
    "#     max_n_terms   = None\n",
    "# else:\n",
    "#     max_n_terms   = int(topic_params['Max Terms'])\n",
    "    \n",
    "# column            = str(topic_params['Column to Vectorize'])\n",
    "# normalize         = bool(int(topic_params['Normalize']))\n",
    "# sublinear_tf      = bool(int(topic_params['Sublinear TF']))\n",
    "# smooth_idf        = bool(int(topic_params['Smooth IDF']))\n",
    "# vocabulary        = topic_params['Vocabulary']\n",
    "# min_df            = float(topic_params['Min DF'])\n",
    "# max_df            = float(topic_params['Max DF'])\n",
    "# min_ic            = float(topic_params['Min IC'])\n",
    "\n",
    "# ## Decomposition Params ##\n",
    "# n_topics          = int(topic_params['Number of Topics'])\n",
    "# model_type        = str(topic_params['Model Type'])\n",
    "\n",
    "# ## Visualization Params ##\n",
    "# top_n_terms       = topic_params['Top Terms Per Topic']\n",
    "# sort_terms_by     = topic_params['Sort Terms By']\n",
    "# term_depth        = topic_params['Depth of Termite Plot']\n",
    "# highlight         = topic_params['Highlight']\n",
    "# directory         = topic_params['Save Directory']\n",
    "\n",
    "# try:\n",
    "#     save          = bool(int(topic_params['Save']))\n",
    "# except:\n",
    "#     save          = str(topic_params['Save'])\n",
    "\n",
    "def stop_fixer(file, upper=True, no_punct=True, fix_in_place=False):\n",
    "    \n",
    "    '''Reads and parses *file* - Returns list of stops or (if fix_in_place) overwrites existing *file*\n",
    "    (if upper) adds capitlized version of stop words\n",
    "    (if no_punct) adds unpunctuated version of stop words'''\n",
    "    \n",
    "    vocab_dict = stop_loader('Stop_Words_List.dat', as_list = False)\n",
    "    \n",
    "    for key in vocab_dict:\n",
    "        if no_punct or upper:\n",
    "            for word in vocab_dict[key]:\n",
    "                if no_punct:\n",
    "                    word_punct = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "                    if word_punct not in vocab_dict[key]:\n",
    "                        vocab_dict[key].append(word_punct)\n",
    "                \n",
    "                if upper:\n",
    "                    word_caps = word.capitalize()\n",
    "                    if word_caps not in vocab_dict[key]:\n",
    "                        vocab_dict[key].append(word_caps)\n",
    "    \n",
    "       \n",
    "        sorted(vocab_dict[key], key=str.lower)\n",
    "        vocab_dict[key] = sorted(vocab_dict[key], key=str.lower)\n",
    "        \n",
    "    if fix_in_place:\n",
    "        \n",
    "        writer = open(file, mode='w+')\n",
    "        \n",
    "        for key in vocab_dict:\n",
    "            writer.writelines('\\n' + key + '\\n')\n",
    "            for word in vocab_dict[key]:\n",
    "                writer.writelines(word  + '\\n')\n",
    "    else:        \n",
    "        return vocab_dict\n",
    "\n",
    "def spacy_stopword_adder():\n",
    "    print(f\"Adding {len(stop_words_list)} custom stop words to Spacy Model {spacy_model}.\")\n",
    "    \n",
    "    if not 'nlp' in globals():\n",
    "        print(f\"Loading Spacy Model {spacy_model}.  This could take a while...\")\n",
    "        global nlp\n",
    "        nlp = spacy.load(spacy_model)\n",
    "        print(\"Complete\")\n",
    "    \n",
    "    for stopword in stop_words_list:\n",
    "        STOP_WORDS.add(stopword)\n",
    "        \n",
    "    nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "    print(f\"Complete. There are {len(STOP_WORDS)} stop words in the list.\")\n",
    "    \n",
    "\n",
    "def mongo_query(host=mongo_host, port=mongo_port, mongo_db=mongo_db, mongo_collection=mongo_collection, \n",
    "                articles=True, custom_query=None, n_hours = n_hours, red_or_blue = red_or_blue, ALL=False):\n",
    "\n",
    "    if custom_query and ALL:\n",
    "        raise ValueError(\"Cannot have custom query and ALL query at once.\")\n",
    "    \n",
    "    client = MongoClient(host=mongo_host, port=mongo_port)\n",
    "    \n",
    "    db = getattr(client, mongo_db)\n",
    "    \n",
    "    collection = getattr(db, mongo_collection)\n",
    "    \n",
    "    if ALL:\n",
    "        df = pd.DataFrame(list(collection.find()))\n",
    "        return df\n",
    "    \n",
    "    if custom_query:\n",
    "        df = pd.DataFrame(list(collection.find(custom_query)))\n",
    "        return df\n",
    "    \n",
    "    if articles:\n",
    "        post = 'articles'\n",
    "    else:\n",
    "        post = 'documents'\n",
    "    \n",
    "    query = {'is article': articles}\n",
    "    \n",
    "    if n_hours == 0:\n",
    "        print(f\"Pulling all articles from {red_or_blue} targets.\")\n",
    "    else:\n",
    "        print(f\"Pulling {red_or_blue} articles from last {n_hours} hours.\")\n",
    "        dt = datetime.utcnow() - timedelta(hours=n_hours)\n",
    "        query['date'] = {'$gt': dt}\n",
    "    \n",
    "    if red_or_blue == 'Red':\n",
    "        query['target'] = True\n",
    "    elif red_or_blue == 'Blue':\n",
    "        query['target'] = False\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(list(collection.find(query)))\n",
    "    \n",
    "    print(f'''Completed pulling {len(df)} {post}.\n",
    "    Latest article is from {collection.find_one(sort=[('date', -1)])['date']} UTC''')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pre_processor(column = 'text'):\n",
    "    \n",
    "    cleaned = [] \n",
    "    for x in range(len(df)):\n",
    "        text = textacy.preprocess.preprocess_text(df[column][x],\n",
    "                                            fix_unicode=fix_unicode,\n",
    "                                            lowercase=lowercase,\n",
    "                                            no_punct=no_punctuation,\n",
    "                                            no_contractions=no_contradictions,\n",
    "                                            no_currency_symbols=no_currency,\n",
    "                                            no_emails=no_emails,\n",
    "                                            no_accents=no_accents)\n",
    "        if no_newline:\n",
    "            text = text.replace('\\n', ' ') \n",
    "        \n",
    "        cleaned.append(text)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def spacy_processing(spacy_model=spacy_model, use_cleaned = use_cleaned, split_columns = split_columns):\n",
    "    \n",
    "    if 'nlp' not in globals():\n",
    "        print(f\"Loading Spacy Model {spacy_model}.  This could take a while...\")\n",
    "        global nlp\n",
    "        nlp = spacy.load(spacy_model)\n",
    "        print(\"Complete\")\n",
    "    \n",
    "    if use_cleaned:\n",
    "        corpus    = 'cleaned'\n",
    "        df_column = 'cleaned'\n",
    "    else:\n",
    "        corpus    = 'raw'\n",
    "        df_column = 'text'   \n",
    "    \n",
    "    print(f'''Filtering stops and words shorter than {min_word_length + 1} letters. \n",
    "Chunking and identifying {named_entities} entities from {corpus} corpus.''')    \n",
    "    \n",
    "    chunks_list = []\n",
    "    ents_list   = []\n",
    "    \n",
    "    for text in df[df_column]:\n",
    "        \n",
    "        doc = nlp(str(text))\n",
    "        \n",
    "        chunks = []\n",
    "        ents   = []\n",
    "        \n",
    "        for span in doc.noun_chunks:\n",
    "            if len(span) == 1:\n",
    "                if span[0].is_stop or len(span[0]) <= min_word_length:\n",
    "                    continue\n",
    "                else:\n",
    "                    chunks.append(span.text)\n",
    "                    continue\n",
    "            else:\n",
    "                chunks.append(span.text)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in named_entities:\n",
    "                ents.append(ent.text)\n",
    "                \n",
    "        chunks_list.append(chunks)\n",
    "        ents_list.append(ents)\n",
    "    \n",
    "    if split_columns:\n",
    "        df['chunks'], df['ents'] = chunks_list, ents_list\n",
    "        del chunks_list, ents_list\n",
    "        print(f'''Done inserting {len(df.chunks.sum())} chunks and\n",
    "              {len(df.ents.sum())} entities into df.chunks and df.ents.''')\n",
    "    else:\n",
    "        joined_list = [a + b for a, b in zip(chunks_list, ents_list)]\n",
    "        del chunks_list, ents_list\n",
    "        df['chunks_ents'] = joined_list\n",
    "        del joined_list\n",
    "        print(f\"Done inserting {len(df.chunks_ents.sum())} chunks and entities into df.chunks_ents.)\")\n",
    "\n",
    "def topic_modeler(save=True, visualize=True, **kwargs):    \n",
    "    \n",
    "    print(f\"Loading Vectorizer Paramaters\")\n",
    "    \n",
    "    vocabulary=None\n",
    "    \n",
    "    vectorizer = textacy.vsm.Vectorizer(weighting, normalize, sublinear_tf,\n",
    "                                        smooth_idf, vocabulary, min_df, max_df,\n",
    "                                        min_ic, max_n_terms)\n",
    "    \n",
    "    print(f\"Fitting Vectorizor on {column} column\")\n",
    "    \n",
    "    doc_term_matrix = vectorizer.fit_transform(df[column])\n",
    "    \n",
    "    model = textacy.tm.TopicModel(model_type, n_topics=n_topics)\n",
    "    model.fit(doc_term_matrix)\n",
    "    \n",
    "    topics_string = \"\"\n",
    "    \n",
    "    for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=top_n_terms):\n",
    "        topics_string += ' - '.join(top_terms) + ' '\n",
    "        print('topic', topic_idx, ':', '   '.join(top_terms))\n",
    "        \n",
    "    if visualize:\n",
    "        if isinstance(save, str):\n",
    "            save = save\n",
    "        elif save is True:\n",
    "            time_stamp = str(utc_to_local(datetime.now()).strftime('%m_%d_%y_%H_%M'))\n",
    "            save = f\"{directory}/{n_hours}hr_{red_or_blue}_{column}_{time_stamp}\"\n",
    "        \n",
    "        model.termite_plot(doc_term_matrix, vectorizer.id_to_term, highlight_topics=highlight,\n",
    "                       topics=-1,  n_terms=term_depth, sort_terms_by=sort_terms_by, save=save)\n",
    "\n",
    "## Import Database from Mongo\n",
    "df = mongo_query()\n",
    "\n",
    "## Preprocessing\n",
    "df['cleaned'] = pre_processor()\n",
    "\n",
    "## Adding Stop Words to spacy\n",
    "spacy_stopword_adder()\n",
    "\n",
    "## Spacy Tokenization and Entity Recognition\n",
    "spacy_processing()\n",
    "\n",
    "## Run Topic Modeler and Visualize Results\n",
    "topic_modeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Library Imports\n",
    "print(\"Loading Libraries\")\n",
    "\n",
    "# Standard Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import pprint\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "from ast import literal_eval\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# MongoDB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# NLP Libraries\n",
    "import textacy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "class Politiprocess:\n",
    "\n",
    "    model_params   = 'Topic_Extractor_Params.dat'\n",
    "    mongo_params   = '../Scraper_Params.dat'\n",
    "    \n",
    "    stop_list = []\n",
    "    \n",
    "    def __init__(self, from_file=True, verbose=False):\n",
    "        print(\"Loading Params\")\n",
    "        \n",
    "        self.params = self.param_loader(self.model_params)\n",
    "        self.mongo  = self.param_loader(self.mongo_params)\n",
    "        \n",
    "        if verbose:\n",
    "            print(self.params)\n",
    "    \n",
    "    def param_loader(self, file):\n",
    "        '''A loader definition for loading script dependant parameters from file'''\n",
    "        \n",
    "        fileObj = open(file, mode='r')\n",
    "        params_dict  = OrderedDict()\n",
    "        \n",
    "        for line in fileObj:\n",
    "            if not line:\n",
    "                pass\n",
    "            line = line.strip()\n",
    "            key_value = line.split('=')\n",
    "    \n",
    "            if \"#\" in line:\n",
    "                section = line[3:].replace(\" \", \"\")\n",
    "                params_dict[section] = {}\n",
    "\n",
    "            if len(key_value) == 2:\n",
    "                key = key_value[0].strip().replace(\" \", \"\")\n",
    "                value = key_value[1].strip()\n",
    " \n",
    "                if not value:\n",
    "                    value = input(f'Required parameter \"{key_value[0].strip()}\" is missing: ')\n",
    "                    while not value:\n",
    "                        value = input(f'You must enter a value for {key_value[0].strip()}: ')\n",
    "                    \n",
    "                \n",
    "                if \",\" in value:\n",
    "                    try:\n",
    "                        params_dict[section][key] = list(literal_eval(value))\n",
    "                    except:\n",
    "                        params_dict[section][key] = value.split(\", \")\n",
    "                else:\n",
    "                    try:\n",
    "                        params_dict[section][key] = literal_eval(value)\n",
    "                    except:\n",
    "                        params_dict[section][key] = value\n",
    "        \n",
    "        param_nt = namedtuple('params', params_dict.keys())(**params_dict)\n",
    "        return param_nt\n",
    "    \n",
    "    def stop_loader(self, file='Stop_Words_List.dat', as_list=True):\n",
    "        '''A loader for loading a unicode file of stop words as dictionaries.\n",
    "        Keys are categories and Values are lists of terms'''\n",
    "        \n",
    "        fileObj = open(file, mode='r')\n",
    "        \n",
    "        vocab_dict = {}\n",
    "        vocab_list = []\n",
    "        current_key = \"\"\n",
    "        \n",
    "        for line in fileObj.read().split('\\n'):\n",
    "            \n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            if \"#\" in line:\n",
    "                vocab_list = []\n",
    "                vocab_dict[line] = []\n",
    "                current_key = str(line)\n",
    "            \n",
    "            if line[0].isalpha():\n",
    "                vocab_list.append(line)\n",
    "            \n",
    "            vocab_dict[current_key] = vocab_list\n",
    "            \n",
    "        if as_list:\n",
    "            vocab_list = []\n",
    "            for terms in vocab_dict.values():\n",
    "                vocab_list.extend(terms)\n",
    "            print(f\"Loading {len(vocab_list)} stop words from {list(vocab_dict.keys())}\")\n",
    "            return vocab_list\n",
    "        \n",
    "        return vocab_dict\n",
    "    \n",
    "    def stop_fixer(self, file='Stop_Words_List.dat', upper=True, no_punct=True, fix_in_place=False):\n",
    "        '''Reads and parses *file* - Returns list of stops or (if fix_in_place) overwrites existing *file*\n",
    "        (if upper) adds capitlized version of stop words\n",
    "        (if no_punct) adds unpunctuated version of stop words'''\n",
    "        \n",
    "        vocab_dict = self.stop_loader('Stop_Words_List.dat', as_list = False)\n",
    "        \n",
    "        for key in vocab_dict:\n",
    "            if no_punct or upper:\n",
    "                for word in vocab_dict[key]:\n",
    "                    if no_punct:\n",
    "                        word_punct = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "                        if word_punct not in vocab_dict[key]:\n",
    "                            vocab_dict[key].append(word_punct)\n",
    "                    \n",
    "                    if upper:\n",
    "                        word_caps = word.capitalize()\n",
    "                        if word_caps not in vocab_dict[key]:\n",
    "                            vocab_dict[key].append(word_caps)\n",
    "        \n",
    "           \n",
    "            sorted(vocab_dict[key], key=str.lower)\n",
    "            vocab_dict[key] = sorted(vocab_dict[key], key=str.lower)\n",
    "            \n",
    "        if fix_in_place:\n",
    "            \n",
    "            writer = open(file, mode='w+')\n",
    "            \n",
    "            for key in vocab_dict:\n",
    "                writer.writelines('\\n' + key + '\\n')\n",
    "                for word in vocab_dict[key]:\n",
    "                    writer.writelines(word  + '\\n')\n",
    "        else:        \n",
    "            return vocab_dict\n",
    "\n",
    "\n",
    "    \n",
    "#     def mongo_query(host=mongo_host, port=mongo_port, mongo_db=mongo_db, mongo_collection=mongo_collection, \n",
    "#                     articles=True, custom_query=None, n_hours = n_hours, red_or_blue = red_or_blue, ALL=False):\n",
    "    \n",
    "#         if custom_query and ALL:\n",
    "#             raise ValueError(\"Cannot have custom query and ALL.\")\n",
    "        \n",
    "#         client = MongoClient(host=mongo_host, port=mongo_port)\n",
    "        \n",
    "#         db = getattr(client, mongo_db)\n",
    "        \n",
    "#         collection = getattr(db, mongo_collection)\n",
    "        \n",
    "#         if ALL:\n",
    "#             df = pd.DataFrame(list(collection.find()))\n",
    "#             return df\n",
    "        \n",
    "#         if custom_query:\n",
    "#             df = pd.DataFrame(list(collection.find(custom_query)))\n",
    "#             return df\n",
    "        \n",
    "#         if articles:\n",
    "#             post = 'articles'\n",
    "#         else:\n",
    "#             post = 'documents'\n",
    "        \n",
    "#         query = {'is article': articles}\n",
    "        \n",
    "#         if n_hours == 0:\n",
    "#             print(f\"Pulling all articles from {red_or_blue} targets.\")\n",
    "#         else:\n",
    "#             print(f\"Pulling {red_or_blue} articles from last {n_hours} hours.\")\n",
    "#             dt = datetime.utcnow() - timedelta(hours=n_hours)\n",
    "#             query['date'] = {'$gt': dt}\n",
    "        \n",
    "#         if red_or_blue == 'Red':\n",
    "#             query['target'] = True\n",
    "#         elif red_or_blue == 'Blue':\n",
    "#             query['target'] = False\n",
    "#         else:\n",
    "#             pass\n",
    "        \n",
    "#         df = pd.DataFrame(list(collection.find(query)))\n",
    "        \n",
    "#         print(f'''Completed pulling {len(df)} {post}.\n",
    "#         Latest article is from {collection.find_one(sort=[('date', -1)])['date']} UTC''')\n",
    "        \n",
    "#         return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Politiprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(topics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=80, width = 800, height=400).generate(topics_string)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud, interpolation=\"bicubic\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
