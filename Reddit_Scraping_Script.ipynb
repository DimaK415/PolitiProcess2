{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Loading Libraries')\n",
    "\n",
    "# Standard Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# URL Parser\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Reddit API\n",
    "import praw\n",
    "\n",
    "# Sentiment and NLP TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Newspaper3k\n",
    "from newspaper import Article\n",
    "\n",
    "# MongoDB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "print('Complete')\n",
    "\n",
    "\n",
    "print('Loading Reddit Params')\n",
    "\n",
    "fileObj = open('Scraper_Params.dat', mode='r')\n",
    "\n",
    "reddit_params = {}\n",
    "\n",
    "for line in fileObj:\n",
    "    line = line.strip()\n",
    "    \n",
    "    key_value = line.split('=')\n",
    "    if len(key_value) == 2:\n",
    "        reddit_params[key_value[0].strip()] = key_value[1].strip()\n",
    "\n",
    "print('Complete')\n",
    "print('Assigning Variables')\n",
    "\n",
    "api = praw.Reddit(client_id=      reddit_params['client_id'],\n",
    "                  client_secret=  reddit_params['client_secret'],\n",
    "                  password=       reddit_params['password'],\n",
    "                  user_agent=     reddit_params['user_agent'],\n",
    "                  username=       reddit_params['username'])\n",
    "\n",
    "red_sub_list = reddit_params['red_list'].strip().split(', ')\n",
    "blu_sub_list = reddit_params['blu_list'].strip().split(', ')\n",
    "\n",
    "sub_limit    = int(reddit_params['limit_per_sub'].strip().split(', ')[0])\n",
    "\n",
    "mongo_host   = str(reddit_params['mongodb_host'])\n",
    "mongo_port   = int(reddit_params['mongodb_port'])\n",
    "\n",
    "print('Complete')\n",
    "\n",
    "def subreddit_title_scraper(df = True):\n",
    "    \n",
    "    global sub_limit\n",
    "    global red_sub_list\n",
    "    global blu_sub_list\n",
    "    global api\n",
    "    \n",
    "    posts_dict = {\"post title\"        : [],\n",
    "                  \"subreddit\"         : [],\n",
    "                  \"score\"             : [],\n",
    "                  \"is article\"        : [],\n",
    "                  \"article title\"     : [],\n",
    "                  \"title polarity\"    : [],\n",
    "                  \"title objectivity\" : [],\n",
    "                  \"keywords\"          : [],\n",
    "                  \"domain\"            : [],\n",
    "                  \"link\"              : [],\n",
    "                  \"author\"            : [],\n",
    "                  \"text\"              : [],\n",
    "                  \"comments\"          : [],\n",
    "                  \"date\"              : [],\n",
    "                  \"target\"            : [],\n",
    "                   }\n",
    "    \n",
    "    article_count = 0\n",
    "    invalid_links = 0\n",
    "    failed_links = 0\n",
    "    \n",
    "    for sub in red_sub_list + blu_sub_list:\n",
    "        submissions = (x for x in api.subreddit(sub).hot(limit=sub_limit) if not x.stickied)\n",
    "        \n",
    "        for post in submissions:\n",
    "            \n",
    "            if sub in red_sub_list:\n",
    "                posts_dict[\"target\"].append(True)\n",
    "            if sub in blu_sub_list:\n",
    "                posts_dict[\"target\"].append(False)\n",
    "           \n",
    "            posts_dict[\"post title\"].append(post.title)           ## praw reddit scraping to dict##\n",
    "            posts_dict[\"link\"].append(post.url)\n",
    "            posts_dict[\"score\"].append(int(post.score))\n",
    "            posts_dict[\"subreddit\"].append(sub)\n",
    "            posts_dict[\"date\"].append(datetime.fromtimestamp(post.created_utc))\n",
    "            \n",
    "            comments = []                                         ## Comments parsing and scoring \n",
    "            for comment in post.comments:\n",
    "                if comment.author != 'AutoModerator':\n",
    "                    comments.append((round(comment.score/(post.num_comments), 2), comment.body))\n",
    "            posts_dict[\"comments\"].append(comments)\n",
    "            \n",
    "            parsed_url = urlparse(post.url)                       ## Parse URL for domain\n",
    "            posts_dict['domain'].append(parsed_url.netloc)\n",
    "            \n",
    "            post_blob = TextBlob(post.title)                      ## TextBlob NLP - VERY SIMPLE\n",
    "            posts_dict[\"title polarity\"].append(post_blob.sentiment[0])\n",
    "            posts_dict[\"title objectivity\"].append(post_blob.sentiment[1])\n",
    "            posts_dict[\"keywords\"].append(post_blob.noun_phrases)\n",
    "            \n",
    "            \n",
    "            article = Article(post.url)                           ## Instantiate newspaper3k library ##\n",
    "            if article.is_valid_url():                            ## Is post a URL?  ##\n",
    "                \n",
    "                try:                                                         ## Try to download and parse article ##\n",
    "                    article.download()\n",
    "                    article.parse()\n",
    "                    \n",
    "                    article_count += 1\n",
    "                    posts_dict[\"is article\"].append(True)\n",
    "                    \n",
    "                    if article.title != []:                                  ## Title parsed? ##\n",
    "                        posts_dict[\"article title\"].append(article.title)\n",
    "                    else:\n",
    "                        posts_dict[\"article title\"].append(np.nan)\n",
    "                    \n",
    "                    if article.authors != []:                                ## Author parsed?  ##\n",
    "                        posts_dict[\"author\"].append(article.authors)\n",
    "                    else:\n",
    "                        posts_dict[\"author\"].append(np.nan)\n",
    "                        \n",
    "                    if article.text != []:                                   ## Text parsed?  ##\n",
    "                        posts_dict['text'].append(article.text)\n",
    "                    else:\n",
    "                        posts_dict[\"text\"].append(np.nan)\n",
    "                    \n",
    "                    if article_count % 5 == 0:\n",
    "                        print(f\"Added {article_count} articles\")\n",
    "\n",
    "                except:                               \n",
    "                    failed_links += 1\n",
    "                    posts_dict[\"is article\"].append(False)\n",
    "                    posts_dict[\"article title\"].append(np.nan)\n",
    "                    posts_dict[\"author\"].append(np.nan)\n",
    "                    posts_dict[\"text\"].append(np.nan)\n",
    "                    \n",
    "                    if invalid_links % 5 == 0:\n",
    "                        print(f\"{failed_links} links failed parse\")\n",
    "                    \n",
    "                    pass\n",
    "            \n",
    "                        \n",
    "            else:\n",
    "                invalid_links += 1\n",
    "                posts_dict[\"is article\"].append(False)\n",
    "                posts_dict[\"article title\"].append(np.nan)\n",
    "                posts_dict[\"author\"].append(np.nan)\n",
    "                posts_dict[\"text\"].append(np.nan)\n",
    "                \n",
    "                if invalid_links % 5 == 0:\n",
    "                        print(f\"{invalid_links} none-articles added\")\n",
    "                \n",
    "                    \n",
    "    if df:\n",
    "        \n",
    "        print(f\"creating data frame from {article_count + invalid_links } links. {failed_links} failed to download\")\n",
    "        \n",
    "        posts_df = pd.DataFrame(posts_dict)                             ## Make it a dataframe ##\n",
    "        posts_df =posts_df[[\"subreddit\",\n",
    "                            \"post title\",\n",
    "                            \"score\",\n",
    "                            \"keywords\",\n",
    "                            \"comments\",\n",
    "                            \"title polarity\",\n",
    "                            \"title objectivity\",\n",
    "                            \"domain\", \n",
    "                            \"link\",\n",
    "                            \"is article\",\n",
    "                            \"article title\",\n",
    "                            \"author\",\n",
    "                            \"text\",\n",
    "                            \"date\", \n",
    "                            \"target\"\n",
    "                           ]]\n",
    "        \n",
    "        print(f\"Done processing {article_count} articles and {invalid_links} non-articles as dataframe\")\n",
    "        \n",
    "        return posts_df\n",
    "                \n",
    "    else:\n",
    "        print(f\"Done processing {article_count} articles and {invalid_links} non-articles as dictionary\")\n",
    "        \n",
    "        return posts_dict\n",
    "\n",
    "print(f\"Pulling {sub_limit} posts from {str(blu_sub_list)} and {str(red_sub_list)}\")\n",
    "\n",
    "df = subreddit_title_scraper(df = True)\n",
    "\n",
    "print('Complete')\n",
    "\n",
    "print('Connecting to MongoDB')\n",
    "\n",
    "client = MongoClient(host=mongo_host, port=mongo_port)\n",
    "\n",
    "db = client.Politiprocess\n",
    "\n",
    "collection = db.reddit_posts\n",
    "\n",
    "print('Connection Established')\n",
    "\n",
    "print('Converting DataFrame to BSON')\n",
    "\n",
    "data = df.to_dict(orient='records')\n",
    "\n",
    "print('Complete')\n",
    "\n",
    "print('Updating Records in Database')\n",
    "\n",
    "old_count = collection.count()\n",
    "\n",
    "for post in data:\n",
    "    collection.update_one({'link': post['link']},{'$set': post}, upsert=True)\n",
    "    \n",
    "new_count = collection.count()\n",
    "\n",
    "added_count = new_count - old_count\n",
    "\n",
    "print(f'Added {added_count} Entries to Database')\n",
    "print('All Processes Completed Succesfully')\n",
    "\n",
    "## Garbage Collection ##\n",
    "\n",
    "data = []\n",
    "df = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
